# Product Requirements Document: Regenerative Content Model & Asset Management

**Version:** 2.0  
**Date:** 2025-01-29  
**Owner:** BMAD Business Analyst  
**Status:** Web Architecture Pivot  
**PRD ID:** PRD-007  
**Dependencies:** Backend Integration Service Layer (PRD-001), Node-Based Production Canvas (PRD-006)

---

## Executive Summary

### Business Justification
The Regenerative Content Model represents a paradigm shift in how generative AI content is managed within collaborative web-based production workflows. Instead of storing large generated files that consume cloud storage and complicate synchronization, this system stores only the creative intent and generation parameters in the PostgreSQL database, while generated content exists as S3 file references that can be recreated on demand by any team member.

This architectural approach solves critical pain points in distributed AI-assisted filmmaking: project databases remain lightweight and responsive, collaboration becomes seamless, creative iterations are unlimited, and projects can evolve with improving AI models. By separating creative decisions from generated outputs, distributed teams gain unprecedented flexibility to iterate, experiment, and refine their vision without the traditional constraints of file management or storage costs.

The regenerative model transforms the project database into a "digital DNA" of the film - a complete blueprint that can regenerate all content while remaining efficient enough for real-time synchronization, instant sharing, and continuous evolution across global teams.

### Target User Personas
- **Distributed Production Teams** - Artists working across time zones needing efficient sync
- **Cloud-Native Studios** - Organizations optimizing cloud storage costs
- **Iterative Creators** - Filmmakers constantly refining with unlimited variations
- **Educational Institutions** - Schools sharing projects without massive transfers
- **Archive-Focused Organizations** - Long-term preservation with minimal storage
- **Model Evolution Adopters** - Teams leveraging improving AI models over time

### Expected Impact on Film Production Workflow
- **Storage Efficiency**: Reduce cloud storage costs by 95%+ through regeneration
- **Collaboration Speed**: Instant project sharing without large file transfers
- **Iteration Freedom**: Unlimited creative variations without storage penalties
- **Global Accessibility**: Work from anywhere without downloading massive assets
- **Future-Proofing**: Projects automatically improve as AI models evolve

---

## Problem Statement

### Current Limitations in Cloud-Based Production
1. **Storage Cost Explosion**: Generated content quickly consumes expensive cloud storage
2. **Synchronization Bottlenecks**: Large files slow down team collaboration
3. **Transfer Limitations**: Sharing projects requires massive bandwidth
4. **Version Proliferation**: Each iteration multiplies storage requirements
5. **Model Lock-In**: Content tied to outdated models becomes stale

### Pain Points in Web-Based Workflows
- **Slow Asset Loading**: Large files create poor user experience
- **Sync Conflicts**: Binary files create irreconcilable merge conflicts
- **Collaboration Delays**: Waiting for large uploads/downloads
- **Storage Quotas**: Teams hit limits with generated content
- **No Regeneration**: Lost files cannot be recreated from project data

### Gaps in Current Web Pipeline
- **No Generation Memory**: Systems don't remember how content was created
- **Missing Parameter Storage**: Generation settings lost after creation
- **No Distributed Regeneration**: Can't leverage team's collective compute
- **Asset Lifecycle Gaps**: No intelligent management of cloud storage
- **Version Evolution**: No mechanism to upgrade content collaboratively

---

## Solution Overview

### Feature Description within Web Architecture
The Regenerative Content Model implements a comprehensive cloud-based asset management system where all creative decisions, parameters, and relationships are stored in PostgreSQL, while generated content exists as S3 references that can be regenerated by any team member at any time. This system treats generated content as a distributed "cache" that can be cleared, moved, or recreated without losing creative intent.

**Core Capabilities:**
1. **Cloud Parameter Storage** - All generation parameters in PostgreSQL with version tracking
2. **S3 Reference Management** - Lightweight URLs instead of embedded media
3. **Distributed Regeneration** - Any team member can regenerate any content
4. **Collaborative Migration** - Teams upgrade content together with new models
5. **Selective Generation** - Generate only what's needed for current work
6. **Smart Caching** - Intelligent distribution of generated content
7. **Storage Analytics** - Track and optimize cloud storage usage
8. **Cross-Region Support** - Regenerate locally for best performance
9. **Batch Operations** - Efficient bulk regeneration capabilities
10. **API Access** - External tools can trigger regeneration
11. **Git LFS Integration** - Binary media tracked in LFS, parameters in Git
12. **Quality Tier Tracking** - Store and regenerate at different quality levels

### Integration with Cloud Architecture
**Database-Driven Parameters:**
- All generation parameters stored in PostgreSQL
- Version tracking for parameter evolution
- Relational links between parameters and outputs
- Query-based regeneration selection
- Quality tier stored with parameters

**Git LFS Integration (from File Structure):**
- Project-as-Repository model with Git + LFS
- Text-based parameters in standard Git
- Generated media files in Git LFS
- Automatic .gitattributes configuration:
  - Images: *.png, *.jpg, *.jpeg → LFS
  - Video: *.mp4, *.mov, *.mkv → LFS
  - Audio: *.wav, *.mp3, *.flac → LFS
  - Models: *.safetensors, *.ckpt → LFS
- Immutable file naming: SHOT-XXX_vYY_takeZZ.mp4
- Selective LFS pull for bandwidth optimization

**S3 Lifecycle Management:**
- Automatic lifecycle policies for generated content
- Tiered storage for frequently/rarely accessed assets
- Cross-region replication for global teams
- Signed URLs for secure access
- LFS backend can use S3 for cloud deployments

**Distributed Processing:**
- Celery tasks for regeneration jobs
- Load balancing across available workers
- Progress tracking via WebSocket
- Collaborative queue management
- Quality-based queue routing

### Backend Service Architecture
**FastAPI Endpoints:**
- Parameter CRUD operations
- Regeneration job submission
- Storage analytics and reporting
- Migration management

**Celery Task Processing:**
- Distributed regeneration tasks
- Batch optimization algorithms
- Progress streaming
- Error recovery

**WebSocket Events:**
- Real-time regeneration progress
- Storage updates
- Migration notifications
- Team activity

---

## User Stories & Acceptance Criteria

### Epic 1: Cloud-Native Regenerative Foundation
**As a distributed team, we want our project to store parameters instead of large files, so we can collaborate efficiently without storage constraints.**

#### User Story 1.1: Parameter-Based Asset Creation
- **Given** team members generate content from anywhere
- **When** generation completes
- **Then** parameters are stored in the shared database
- **And** only S3 references are saved
- **And** database remains performant
- **And** all team members can regenerate
- **And** Git LFS tracks the media files
- **And** quality tier is preserved

**Acceptance Criteria:**
- Complete parameter capture in PostgreSQL
- S3 URLs stored as references
- <100ms database query performance
- Regeneration available to all users
- Git LFS pointers created automatically
- Quality tier stored and retrievable

#### User Story 1.2: Global Project Portability
- **Given** teams working across continents
- **When** sharing projects
- **Then** only database records transfer
- **And** team members regenerate locally
- **And** creative intent perfectly preserved
- **And** no large file transfers needed

**Acceptance Criteria:**
- Instant project sharing via database
- Local regeneration from any region
- Consistent results across locations
- Zero large file transfers

### Epic 2: Collaborative Regeneration
**As a production team, we want to regenerate content collaboratively, so we can optimize resources and share the workload.**

#### User Story 2.1: Distributed Regeneration Queue
- **Given** multiple assets need regeneration
- **When** team members are available
- **Then** work is distributed intelligently
- **And** progress visible to all
- **And** results automatically shared
- **And** no duplicate work occurs

**Acceptance Criteria:**
- Intelligent work distribution
- Real-time progress visibility
- Automatic result sharing
- Deduplication logic

#### User Story 2.2: Selective Team Regeneration
- **Given** specific scenes need updating
- **When** team decides what to regenerate
- **Then** members can claim tasks
- **And** collaborate on priorities
- **And** share computational load
- **And** coordinate efficiently

**Acceptance Criteria:**
- Task claiming system
- Priority voting mechanism
- Load balancing algorithms
- Coordination tools

### Epic 3: Model Evolution Management
**As a studio, we want to upgrade content collaboratively as models improve, while maintaining creative consistency.**

#### User Story 3.1: Collaborative Migration Planning
- **Given** new models available
- **When** team evaluates upgrades
- **Then** preview impacts together
- **And** vote on migrations
- **And** plan resource allocation
- **And** execute coordinatedly

**Acceptance Criteria:**
- Shared preview system
- Team voting mechanism
- Resource planning tools
- Coordinated execution

#### User Story 3.2: Distributed Migration Execution
- **Given** team approves migration
- **When** executing upgrade
- **Then** work distributed across team
- **And** progress tracked centrally
- **And** results validated together
- **And** rollback available if needed
- **And** Git history preserved
- **And** quality tiers maintained

**Acceptance Criteria:**
- Distributed execution system
- Central progress tracking
- Collaborative validation
- Team rollback capability
- Git commit for each migration
- Quality tier upgrade options

### Epic 4: Intelligent Storage Management
**As a cloud-conscious team, we want smart storage management, so we can minimize costs while maximizing availability.**

#### User Story 4.1: Usage Analytics Dashboard
- **Given** ongoing productions
- **When** monitoring storage
- **Then** see team usage patterns
- **And** identify optimization opportunities
- **And** track cost trends
- **And** make informed decisions

**Acceptance Criteria:**
- Real-time usage dashboard
- Cost projection tools
- Optimization recommendations
- Historical trending

#### User Story 4.2: Collaborative Cleanup
- **Given** storage optimization needed
- **When** team reviews content
- **Then** vote on cleanup candidates
- **And** ensure safe deletion
- **And** coordinate regeneration
- **And** track savings achieved

**Acceptance Criteria:**
- Voting system for cleanup
- Safety validation
- Regeneration coordination
- Savings tracking

### Epic 5: Cross-Region Performance
**As a global team, we want region-optimized regeneration, so everyone has fast access regardless of location.**

#### User Story 5.1: Regional Caching Strategy
- **Given** team members worldwide
- **When** accessing content
- **Then** regenerate in nearest region
- **And** cache intelligently
- **And** share when beneficial
- **And** minimize latency

**Acceptance Criteria:**
- Region detection
- Smart routing logic
- Selective caching
- Latency optimization

#### User Story 5.2: Bandwidth Optimization
- **Given** varying connection speeds
- **When** team members work
- **Then** prioritize essential content
- **And** use progressive loading
- **And** optimize transfers
- **And** ensure smooth experience

**Acceptance Criteria:**
- Bandwidth detection
- Progressive loading
- Transfer optimization
- Experience metrics

---

## Technical Requirements

### Web Application Architecture

#### 1. Frontend Parameter Management Requirements

**Regenerative Asset Component Requirements:**
- Capture all generation parameters including quality tier
- Store parameters separate from generated content
- Track regeneration status and progress
- Support quality tier selection during regeneration
- Display storage usage information

**Parameter Capture Requirements:**
- Timestamp of generation
- Model version used
- All input parameters
- Workflow/pipeline configuration
- Random seed for reproducibility
- Quality tier (low/standard/high)
- Backend-specific settings

**Regeneration Interface Requirements:**
- Quality tier selector with current tier highlighted
- Estimated time/cost per quality tier
- Progress indicator during regeneration
- Cancel/pause functionality
- Comparison view (old vs new)
- Batch selection for multiple assets

#### 2. API Endpoint Requirements

**Parameter Storage Endpoint Requirements:**
- Store complete generation parameters with version tracking
- Include quality tier in parameter record
- Link to S3/file references
- Track user who created the asset
- Update storage analytics
- Trigger Git LFS tracking for media files
- Support parameter migration between versions

**Regeneration Queue Endpoint Requirements:**
- Accept quality tier selection (may differ from original)
- Implement deduplication logic
- Calculate priority based on user tier and quality
- Route to appropriate worker queue by quality:
  - Low → fast_regeneration queue
  - Standard → balanced_regeneration queue
  - High → premium_regeneration queue
- Create job tracking record
- Notify team via WebSocket
- Estimate completion time based on quality

**Git Integration Requirements:**
- Auto-commit regenerated files to Git LFS
- Meaningful commit messages with quality tier
- Track parameter changes in Git history
- Support rollback to previous versions

#### 3. Regeneration Processing Requirements

**Quality-Based Regeneration Flow:**
1. Extract quality tier from options (default to original)
2. Check if model migration needed for parameters
3. Select appropriate pipeline based on quality:
   - Low → Fast pipeline with reduced settings
   - Standard → Balanced pipeline
   - High → Premium pipeline with maximum quality
4. Route to appropriate worker based on region and resources
5. Execute generation with progress tracking
6. Store output following file structure conventions
7. Update Git LFS tracking
8. Manage storage lifecycle

**File Storage Integration:**
- Use project file structure paths (PRD-008)
- Follow immutable naming: ASSET-NAME_vXX_takeYY.ext
- Store in appropriate project directory:
  - Characters → 01_Assets/Generative_Assets/Characters/
  - Styles → 01_Assets/Generative_Assets/Styles/
  - Shots → 03_Renders/SCENE/SHOT/
- Automatic Git LFS tracking for media
- Parameter JSON files in standard Git

**Quality-Specific Processing:**
- **Low Quality**: 
  - Timeout: 2 minutes
  - Priority: High (fast turnaround)
  - Cleanup: Aggressive (7 days)
- **Standard Quality**:
  - Timeout: 5 minutes
  - Priority: Normal
  - Cleanup: Balanced (30 days)
- **High Quality**:
  - Timeout: 10 minutes
  - Priority: Low (resource intensive)
  - Cleanup: Conservative (90 days)

#### 4. Storage Management System Requirements

**Cloud Storage Manager Capabilities:**
- Manage S3/file storage for regenerative content
- Support multiple regions with client initialization
- Apply lifecycle policies based on quality and age
- Analyze usage patterns for optimization
- Git LFS integration for version control

**Usage Analytics Requirements:**
- Track total storage size by organization
- Breakdown by region, type, and age
- Distinguish regeneratable vs permanent content
- Calculate potential savings from cleanup
- Track quality tier distribution
- Monitor Git LFS storage separately

**Storage Optimization Strategies:**
- **Aggressive (Low Quality Default)**:
  - Delete regeneratable content >7 days old
  - Keep only latest version
  - Minimal Git history
  
- **Balanced (Standard Quality Default)**:
  - Delete regeneratable content >30 days old
  - Keep 2 recent versions
  - Standard Git history
  
- **Conservative (High Quality Default)**:
  - Delete only explicitly marked content
  - Keep all versions for 90 days
  - Full Git history with LFS

**Cleanup Safety Requirements:**
- Verify content is regeneratable before deletion
- Check for active references
- Ensure parameters are preserved
- Maintain Git LFS pointers
- Team notification before bulk cleanup

### Database Schema Extensions

#### PostgreSQL Schema Requirements for Regenerative Model

**Asset Parameters Table Requirements:**
- Store complete generation parameters as JSONB
- Track model version and quality tier
- Reference to S3/file storage URL
- File size for analytics
- Regeneration capability flag
- Creation and access timestamps
- User attribution
- Git commit hash reference

**Parameter Version History Requirements:**
- Track all parameter versions for an asset
- Version numbering system
- Complete parameter snapshots
- Model version at time of generation
- Migration tracking (from/to versions)
- Quality tier for each version
- User and timestamp tracking

**Regeneration Jobs Table Requirements:**
- Unique job identifier
- Asset reference
- Job status (queued, processing, completed, failed)
- Priority based on quality tier
- Worker assignment
- Time tracking (start, complete)
- Error capture
- Result storage location
- Quality tier selection

**Storage Analytics Table Requirements:**
- Organization-level metrics
- Total storage size tracking
- Breakdown by regeneratable vs permanent
- Regional distribution
- Asset type distribution
- Quality tier distribution
- Cost estimates
- Git LFS vs S3 breakdown

**Cleanup History Table Requirements:**
- Track all cleanup operations
- Organization scope
- User who initiated
- Strategy used (aggressive/balanced/conservative)
- Number of assets affected
- Space recovered
- Execution timestamp
- Quality tier breakdown

### Performance Optimizations

#### 1. Smart Caching Strategy
- Regional edge caching for frequently accessed content
- Predictive pre-generation based on usage patterns
- Collaborative cache sharing between team members
- Automatic cache invalidation on parameter updates

#### 2. Distributed Processing
- Load balancing across global GPU workers
- Regional affinity for optimal performance
- Batch optimization for related assets
- Priority queuing based on user tier

#### 3. Storage Optimization
- Automatic compression for archived content
- Tiered storage with lifecycle transitions
- Deduplication across similar generations
- Smart cleanup recommendations

---

## Success Metrics

### Storage Efficiency
**Primary KPIs:**
- **Cost Reduction**: >90% reduction in storage costs
- **Transfer Efficiency**: <1GB average project transfer
- **Regeneration Speed**: <3 minutes average per asset
- **Cache Hit Rate**: >70% content available instantly

**Measurement Methods:**
- Monthly storage cost analysis
- Transfer size monitoring
- Regeneration timing analytics
- Cache performance metrics

### Collaboration Enhancement
**Team Metrics:**
- **Sharing Speed**: 100x faster project sharing
- **Global Access**: <5s to access any project
- **Concurrent Work**: 10+ team members without conflicts
- **Version Adoption**: >50% using latest models

**System Metrics:**
- Sharing time measurements
- Access latency monitoring
- Concurrency analytics
- Model version tracking

### Reliability and Trust
**System Reliability:**
- **Regeneration Success**: >99.5% successful
- **Parameter Integrity**: Zero corruption incidents
- **Availability**: 99.9% uptime for regeneration
- **Recovery Rate**: 100% from storage failures

**Quality Metrics:**
- Automated regeneration testing
- Parameter validation monitoring
- System availability tracking
- Disaster recovery testing

---

## Risk Assessment & Mitigation

### Technical Risks

#### High Risk: Parameter Synchronization
- **Risk**: Parameters could desync across regions
- **Impact**: Inconsistent regeneration results
- **Mitigation**: 
  - Strong consistency database settings
  - Parameter versioning system
  - Checksum validation
  - Sync verification tools

#### Medium Risk: Regeneration Variations
- **Risk**: Results differ from originals
- **Impact**: User trust issues
- **Mitigation**:
  - Deterministic generation
  - Visual diff tools
  - A/B comparison features
  - Clear communication

### Business Risks

#### High Risk: Cost Overruns
- **Risk**: Regeneration costs exceed storage savings
- **Impact**: Economic model failure
- **Mitigation**:
  - Smart caching strategies
  - Usage-based regeneration
  - Cost monitoring alerts
  - Optimization algorithms

---

## Implementation Roadmap

### Phase 1: Cloud Foundation (Weeks 1-4)
**Deliverables:**
- Database schema for parameters
- S3 integration with references
- Basic regeneration API
- Simple web interface

**Success Criteria:**
- Parameters stored correctly
- S3 references working
- Basic regeneration functional
- Team access verified

### Phase 2: Collaborative Features (Weeks 5-8)
**Deliverables:**
- Distributed regeneration queue
- Team coordination tools
- Progress synchronization
- Usage analytics

**Success Criteria:**
- Queue distribution working
- Team coordination smooth
- Real-time progress updates
- Analytics providing insights

### Phase 3: Optimization Systems (Weeks 9-12)
**Deliverables:**
- Smart caching layer
- Regional optimization
- Storage management tools
- Cost tracking

**Success Criteria:**
- Cache performance improved
- Regional access optimized
- Storage costs reduced
- Cost visibility achieved

### Phase 4: Advanced Features (Weeks 13-16)
**Deliverables:**
- Model migration system
- Batch operations
- Advanced analytics
- API documentation

**Success Criteria:**
- Migrations working smoothly
- Batch efficiency proven
- Analytics driving decisions
- API fully documented

---

## Stakeholder Sign-Off

### Development Team Approval
- [ ] **Frontend Lead** - Web interface design approved
- [ ] **Backend Lead** - API architecture validated
- [ ] **Infrastructure Lead** - Cloud strategy confirmed
- [ ] **DevOps Lead** - Deployment plan accepted

### Business Stakeholder Approval
- [ ] **Product Owner** - Cost model validated
- [ ] **Finance** - Economic benefits confirmed
- [ ] **Customer Success** - User value demonstrated
- [ ] **Marketing** - Differentiators identified

---

**Next Steps:**
1. Design parameter schema with versioning
2. Create S3 lifecycle policies
3. Build regeneration queue system
4. Plan cost optimization strategies
5. Develop migration tools

---

*This PRD represents the transformation of the regenerative model from a desktop-centric approach to a cloud-native collaborative system, enabling global teams to work with unlimited creative iterations while minimizing storage costs and maximizing performance through intelligent distributed processing.*